{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Validation Analysis Example\n",
    "\n",
    "This notebook demonstrates how to perform external validation of health economic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts directory to the path\n",
    "sys.path.append(os.path.join(os.pardir, 'scripts'))\n",
    "sys.path.append(os.path.join(os.pardir, 'scripts', 'models'))\n",
    "sys.path.append(os.path.join(os.pardir, 'scripts', 'core'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define external validation functions\n",
    "def perform_external_validation(\n",
    "    model_predictions,\n",
    "    external_data,\n",
    "    strategy,\n",
    "    metric='rmse'\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform external validation of model predictions against external data.\n",
    "    \n",
    "    Args:\n",
    "        model_predictions: Predicted outcomes from the model\n",
    "        external_data: Observed outcomes from external source\n",
    "        strategy: Treatment strategy being validated\n",
    "        metric: Validation metric ('rmse', 'mae', 'mape', 'r_squared')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Calculate validation metrics\n",
    "    predictions = np.array(model_predictions)\n",
    "    observations = np.array(external_data)\n",
    "    \n",
    "    # Calculate differences\n",
    "    diffs = predictions - observations\n",
    "    abs_diffs = np.abs(diffs)\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    mse = np.mean(diffs ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(abs_diffs)\n",
    "    mape = np.mean(np.abs(diffs / (observations + 1e-8))) * 100  # Add small value to avoid division by zero\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    ss_res = np.sum((observations - predictions) ** 2)\n",
    "    ss_tot = np.sum((observations - np.mean(observations)) ** 2)\n",
    "    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "    \n",
    "    # Calculate concordance correlation coefficient (CCC)\n",
    "    mean_pred = np.mean(predictions)\n",
    "    mean_obs = np.mean(observations)\n",
    "    var_pred = np.var(predictions)\n",
    "    var_obs = np.var(observations)\n",
    "    cov_pred_obs = np.cov(predictions, observations)[0, 1]\n",
    "    \n",
    "    ccc = (2 * cov_pred_obs) / (var_pred + var_obs + (mean_pred - mean_obs) ** 2)\n",
    "    \n",
    "    # Determine validation category\n",
    "    if r_squared >= 0.8:\n",
    "        validation_category = 'Excellent'\n",
    "    elif r_squared >= 0.6:\n",
    "        validation_category = 'Good'\n",
    "    elif r_squared >= 0.4:\n",
    "        validation_category = 'Acceptable'\n",
    "    elif r_squared >= 0.2:\n",
    "        validation_category = 'Poor'\n",
    "    else:\n",
    "        validation_category = 'Very Poor'\n",
    "    \n",
    "    return {\n",
    "        'strategy': strategy,\n",
    "        'predictions': predictions,\n",
    "        'observations': observations,\n",
    "        'differences': diffs,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r_squared': r_squared,\n",
    "        'ccc': ccc,\n",
    "        'validation_category': validation_category,\n",
    "        'n_observations': len(observations),\n",
    "        'mean_prediction': np.mean(predictions),\n",
    "        'mean_observation': np.mean(observations),\n",
    "        'bias': np.mean(diffs)  # Systematic bias\n",
    "    }\n",
    "\n",
    "def assess_calibration(\n",
    "    model_predictions,\n",
    "    external_data,\n",
    "    n_bins=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Assess calibration of model predictions against external data.\n",
    "    \n",
    "    Args:\n",
    "        model_predictions: Predicted outcomes from the model\n",
    "        external_data: Observed outcomes from external source\n",
    "        n_bins: Number of bins to use for calibration assessment\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with calibration assessment by bin\n",
    "    \"\"\"\n",
    "    predictions = np.array(model_predictions)\n",
    "    observations = np.array(external_data)\n",
    "    \n",
    "    # Create bins based on predicted values\n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_edges = np.quantile(predictions, quantiles)\n",
    "    \n",
    "    bin_results = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (predictions >= bin_edges[i]) & (predictions < bin_edges[i+1])\n",
    "        if i == n_bins - 1:  # Include upper boundary for last bin\n",
    "            mask = (predictions >= bin_edges[i]) & (predictions <= bin_edges[i+1])\n",
    "        \n",
    "        pred_bin = predictions[mask]\n",
    "        obs_bin = observations[mask]\n",
    "        \n",
    "        if len(pred_bin) > 0:\n",
    "            mean_pred = np.mean(pred_bin)\n",
    "            mean_obs = np.mean(obs_bin)\n",
    "            n_records = len(pred_bin)\n",
    "            \n",
    "            bin_results.append({\n",
    "                'bin': i,\n",
    "                'bin_range': f'{bin_edges[i]:.3f}-{bin_edges[i+1]:.3f}',\n",
    "                'mean_predicted': mean_pred,\n",
    "                'mean_observed': mean_obs,\n",
    "                'n_records': n_records,\n",
    "                'calibration_ratio': mean_obs / (mean_pred + 1e-8)  # Avoid division by zero\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(bin_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define mock model predictions and external validation data\n",
    "# In a real scenario, these would come from the model and external studies\n",
    "strategies = ['ECT', 'IV-KA', 'PO-KA']\n",
    "n_patients = 100  # Simulated patient-level data\n",
    "\n",
    "# Generate mock predictions and external data\n",
    "validation_data = []\n",
    "for strategy in strategies:\n",
    "    # Simulate model predictions (e.g., QALYs gained over 1 year)\n",
    "    if strategy == 'ECT':\n",
    "        # ECT: Moderate outcomes with some uncertainty\n",
    "        base_pred = 0.45\n",
    "        predictions = np.random.normal(base_pred, 0.1, n_patients)\n",
    "        # External data with slight systematic difference\n",
    "        observations = predictions + np.random.normal(0.01, 0.08, n_patients)\n",
    "    elif strategy == 'IV-KA':\n",
    "        # IV-KA: Higher efficacy with more certainty\n",
    "        base_pred = 0.65\n",
    "        predictions = np.random.normal(base_pred, 0.08, n_patients)\n",
    "        # External data closer to predictions\n",
    "        observations = predictions + np.random.normal(-0.01, 0.07, n_patients)\n",
    "    else:  # PO-KA\n",
    "        # PO-KA: Good efficacy with some uncertainty\n",
    "        base_pred = 0.60\n",
    "        predictions = np.random.normal(base_pred, 0.09, n_patients)\n",
    "        # External data with more variability\n",
    "        observations = predictions + np.random.normal(0.02, 0.1, n_patients)\n",
    "    \n",
    "    # Ensure positive values for utilities/QALYs\n",
    "    predictions = np.clip(predictions, 0.01, 1.0)\n",
    "    observations = np.clip(observations, 0.01, 1.0)\n",
    "    \n",
    "    validation_data.append({\n",
    "        'strategy': strategy,\n",
    "        'predictions': predictions,\n",
    "        'observations': observations\n",
    "    })\n",
    "\n",
    "# Perform external validation for each strategy\n",
    "validation_results = []\n",
    "calibration_results = []\n",
    "\n",
    "for data in validation_data:\n",
    "    # Perform validation\n",
    "    result = perform_external_validation(\n",
    "        data['predictions'],\n",
    "        data['observations'],\n",
    "        data['strategy']\n",
    "    )\n",
    "    validation_results.append(result)\n",
    "    \n",
    "    # Assess calibration\n",
    "    cal_result = assess_calibration(\n",
    "        data['predictions'],\n",
    "        data['observations']\n",
    "    )\n",
    "    cal_result['strategy'] = data['strategy']\n",
    "    calibration_results.append(cal_result)\n",
    "\n",
    "# Create validation summary DataFrame\n",
    "validation_summary = pd.DataFrame([\n",
    "    {\n",
    "        'strategy': r['strategy'],\n",
    "        'rmse': r['rmse'],\n",
    "        'mae': r['mae'],\n",
    "        'mape': r['mape'],\n",
    "        'r_squared': r['r_squared'],\n",
    "        'ccc': r['ccc'],\n",
    "        'validation_category': r['validation_category'],\n",
    "        'bias': r['bias'],\n",
    "        'mean_prediction': r['mean_prediction'],\n",
    "        'mean_observation': r['mean_observation']\n",
    "    } for r in validation_results\n",
    "])\n",
    "\n",
    "print(\"EXTERNAL VALIDATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(validation_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize external validation results\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Observed vs Predicted Scatter Plots (one per strategy)\n",
    "colors = ['blue' if s == 'ECT' else 'green' if s == 'IV-KA' else 'orange' for s in validation_summary['strategy']]\n",
    "for i, result in enumerate(validation_results):\n",
    "    ax[0, 0].scatter(result['predictions'], result['observations'], \n",
    "                     label=result['strategy'], alpha=0.6, color=colors[i])\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(min(r['predictions']) for r in validation_results)\n",
    "max_val = max(max(r['predictions']) for r in validation_results)\n",
    "perfect_line = np.linspace(min_val, max_val, 100)\n",
    "ax[0, 0].plot(perfect_line, perfect_line, 'r--', label='Perfect Prediction', linewidth=2)\n",
    "\n",
    "ax[0, 0].set_xlabel('Predicted QALYs')\n",
    "ax[0, 0].set_ylabel('Observed QALYs')\n",
    "ax[0, 0].set_title('Model Validation: Predicted vs Observed QALYs')\n",
    "ax[0, 0].legend()\n",
    "ax[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Metrics Comparison\n",
    "x = np.arange(len(validation_summary))\n",
    "width = 0.15\n",
    "\n",
    "ax[0, 1].bar(x - width*1.5, validation_summary['rmse'], width, label='RMSE', alpha=0.7)\n",
    "ax[0, 1].bar(x - width/2, validation_summary['mae'], width, label='MAE', alpha=0.7)\n",
    "ax[0, 1].bar(x + width/2, validation_summary['r_squared'], width, label='R²', alpha=0.7)\n",
    "ax[0, 1].bar(x + width*1.5, validation_summary['ccc'], width, label='CCC', alpha=0.7)\n",
    "ax[0, 1].set_xlabel('Strategy')\n",
    "ax[0, 1].set_ylabel('Metric Value')\n",
    "ax[0, 1].set_title('Validation Metrics Comparison')\n",
    "ax[0, 1].set_xticks(x)\n",
    "ax[0, 1].set_xticklabels(validation_summary['strategy'])\n",
    "ax[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Bias Assessment\n",
    "bias_colors = ['red' if abs(b) > 0.05 else 'green' for b in validation_summary['bias']]\n",
    "bars = ax[1, 0].bar(validation_summary['strategy'], validation_summary['bias'], color=bias_colors, alpha=0.7)\n",
    "ax[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.7)\n",
    "ax[1, 0].set_ylabel('Bias (Pred - Obs)')\n",
    "ax[1, 0].set_title('Prediction Bias by Strategy (Red if >0.05)')\n",
    "ax[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, bias_val in zip(bars, validation_summary['bias']):\n",
    "    height = bar.get_height()\n",
    "    ax[1, 0].text(bar.get_x() + bar.get_width()/2., height + (0.001 if height >= 0 else -0.005),\n",
    "                 f'{bias_val:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "# 4. MAPE by Strategy\n",
    "bars2 = ax[1, 1].bar(validation_summary['strategy'], validation_summary['mape'], color=colors, alpha=0.7)\n",
    "ax[1, 1].set_ylabel('Mean Absolute Percentage Error (%)')\n",
    "ax[1, 1].set_title('MAPE by Strategy')\n",
    "ax[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mape_val in zip(bars2, validation_summary['mape']):\n",
    "    height = bar.get_height()\n",
    "    ax[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(validation_summary['mape'])*0.01,\n",
    "                 f'{mape_val:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize calibration assessment\n",
    "fig, ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "ax = ax.flatten()  # Flatten to iterate easily\n",
    "\n",
    "for i, cal_result in enumerate(calibration_results):\n",
    "    if not cal_result.empty:\n",
    "        strategy = cal_result['strategy'].iloc[0]\n",
    "        \n",
    "        # Calibration plot: Mean observed vs mean predicted by bin\n",
    "        ax[i].scatter(cal_result['mean_predicted'], cal_result['mean_observed'], \n",
    "                     s=100, alpha=0.7, color=['blue', 'green', 'orange'][i], label=strategy)\n",
    "        \n",
    "        # Add perfect calibration line\n",
    "        min_val = min(min(cal_result['mean_predicted']), min(cal_result['mean_observed']))\n",
    "        max_val = max(max(cal_result['mean_predicted']), max(cal_result['mean_observed']))\n",
    "        perfect_line = np.linspace(min_val, max_val, 100)\n",
    "        ax[i].plot(perfect_line, perfect_line, 'r--', label='Perfect Calibration', linewidth=1)\n",
    "        \n",
    "        ax[i].set_xlabel('Mean Predicted')\n",
    "        ax[i].set_ylabel('Mean Observed')\n",
    "        ax[i].set_title(f'Calibration Plot: {strategy}')\n",
    "        ax[i].legend()\n",
    "        ax[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add bin labels\n",
    "        for j, row in cal_result.iterrows():\n",
    "            ax[i].annotate(f'Bin {row[\"bin\"]}\\nN={row[\"n_records\"]}',\n",
    "                          (row['mean_predicted'], row['mean_observed']),\n",
    "                          textcoords=\"offset points\", xytext=(5,5), ha='left')\n",
    "    else:\n",
    "        ax[i].text(0.5, 0.5, f'No calibration\\ndata for {strategies[i]}', \n",
    "                  horizontalalignment='center', verticalalignment='center',\n",
    "                  transform=ax[i].transAxes, fontsize=12)\n",
    "        ax[i].set_title(f'Calibration Plot: {strategies[i]}')\n",
    "\n",
    "# Hide the last subplot if we have only 3 strategies\n",
    "if len(ax) > 3:\n",
    "    ax[3].set_visible(False)\n",
    "    ax[4].set_visible(False)\n",
    "    ax[5].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed calibration results\n",
    "print(\"CALIBRATION ASSESSMENT BY STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "for i, result in enumerate(calibration_results):\n",
    "    print(f\"\\n{result['strategy'].iloc[0] if not result.empty else 'None'}:\\n\" + \"-\"*30)\n",
    "    if not result.empty:\n",
    "        print(result[['bin', 'bin_range', 'n_records', 'mean_predicted', 'mean_observed', 'calibration_ratio']].round(4))\n",
    "    else:\n",
    "        print(\"  No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create validation summary with interpretability\n",
    "print(\"EXTERNAL VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for result in validation_results:\n",
    "    print(f\"\\n{result['strategy']}:\\n\" + \"-\"*30)\n",
    "    print(f\"  N Observations: {result['n_observations']}\")\n",
    "    print(f\"  Mean Prediction: {result['mean_prediction']:.3f}\")\n",
    "    print(f\"  Mean Observation: {result['mean_observation']:.3f}\")\n",
    "    print(f\"  Prediction Bias: {result['bias']:.3f}\")\n",
    "    print(f\"  RMSE: {result['rmse']:.3f}\")\n",
    "    print(f\"  MAE: {result['mae']:.3f}\")\n",
    "    print(f\"  MAPE: {result['mape']:.2f}%\")\n",
    "    print(f\"  R-squared: {result['r_squared']:.3f}\")\n",
    "    print(f\"  Concordance Correlation (CCC): {result['ccc']:.3f}\")\n",
    "    print(f\"  Validation Category: {result['validation_category']}\")\n",
    "    \n",
    "    # Assess validity\n",
    "    issues = []\n",
    "    if abs(result['bias']) > 0.05:\n",
    "        issues.append(f\"Significant bias ({result['bias']:.3f})\")\n",
    "    if result['r_squared'] < 0.5:\n",
    "        issues.append(f\"Low R² ({result['r_squared']:.3f})\")\n",
    "    if result['mape'] > 20:\n",
    "        issues.append(f\"High MAPE ({result['mape']:.1f}%)\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  Issues: {', '.join(issues)}\")\n",
    "    else:\n",
    "        print(f\"  No significant validation issues detected\")\n",
    "\n",
    "# Identify most and least validated strategies\n",
    "best_r2 = max(validation_results, key=lambda x: x['r_squared'])\n",
    "worst_r2 = min(validation_results, key=lambda x: x['r_squared'])\n",
    "\n",
    "print(f\"\\n\\nVALIDATION ASSESSMENT:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Best Validated Strategy (R²): {best_r2['strategy']} (R²={best_r2['r_squared']:.3f})\")\n",
    "print(f\"Least Validated Strategy (R²): {worst_r2['strategy']} (R²={worst_r2['r_squared']:.3f})\")\n",
    "\n",
    "# Identify strategies with systematic issues\n",
    "high_bias_strategies = [r for r in validation_results if abs(r['bias']) > 0.05]\n",
    "poor_r2_strategies = [r for r in validation_results if r['r_squared'] < 0.5]\n",
    "\n",
    "if high_bias_strategies:\n",
    "    print(f\"\\nStrategies with significant bias (>0.05): {[s['strategy'] for s in high_bias_strategies]}\")\n",
    "    \n",
    "if poor_r2_strategies:\n",
    "    print(f\"Strategies with poor fit (R²<0.5): {[s['strategy'] for s in poor_r2_strategies]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cross-validation-style assessment (leave-one-out approach)\n",
    "def perform_cv_validation(all_predictions, all_observations, n_splits=5):\n",
    "    \"\"\"Simulate cross-validation by splitting data into folds.\"\"\"\n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(all_predictions):\n",
    "        train_pred, test_pred = all_predictions[train_idx], all_predictions[test_idx]\n",
    "        train_obs, test_obs = all_observations[train_idx], all_observations[test_idx]\n",
    "        \n",
    "        # In a real scenario, we would retrain on train data\n",
    "        # Here we'll just validate on test data\n",
    "        result = perform_external_validation(test_pred, test_obs, 'CV-Fold')\n",
    "        fold_results.append(result)\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "# Perform cross-validation assessment for the most validated strategy\n",
    "best_validated_idx = validation_summary['r_squared'].idxmax()\n",
    "best_strategy_data = validation_data[best_validated_idx]\n",
    "\n",
    "cv_results = perform_cv_validation(\n",
    "    best_strategy_data['predictions'], \n",
    "    best_strategy_data['observations']\n",
    ")\n",
    "\n",
    "# Calculate CV metrics\n",
    "cv_metrics = {\n",
    "    'rmse_mean': np.mean([r['rmse'] for r in cv_results]),\n",
    "    'rmse_std': np.std([r['rmse'] for r in cv_results]),\n",
    "    'mae_mean': np.mean([r['mae'] for r in cv_results]),\n",
    "    'mae_std': np.std([r['mae'] for r in cv_results]),\n",
    "    'r_squared_mean': np.mean([r['r_squared'] for r in cv_results]),\n",
    "    'r_squared_std': np.std([r['r_squared'] for r in cv_results])\n",
    "}\n",
    "\n",
    "print(f\"\\nCROSS-VALIDATION ASSESSMENT (for best-validated strategy: {best_strategy_data['strategy']}):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  RMSE (mean ± std): {cv_metrics['rmse_mean']:.3f} ± {cv_metrics['rmse_std']:.3f}\")\n",
    "print(f\"  MAE (mean ± std): {cv_metrics['mae_mean']:.3f} ± {cv_metrics['mae_std']:.3f}\")\n",
    "print(f\"  R² (mean ± std): {cv_metrics['r_squared_mean']:.3f} ± {cv_metrics['r_squared_std']:.3f}\")\n",
    "\n",
    "# Confidence intervals\n",
    "from scipy import stats\n",
    "alpha = 0.05  # 95% confidence\n",
    "n_folds = len(cv_results)\n",
    "t_val = stats.t.ppf(1 - alpha/2, n_folds - 1)\n",
    "\n",
    "rmse_se = cv_metrics['rmse_std'] / np.sqrt(n_folds)\n",
    "mae_se = cv_metrics['mae_std'] / np.sqrt(n_folds)\n",
    "r2_se = cv_metrics['r_squared_std'] / np.sqrt(n_folds)\n",
    "\n",
    "print(f\"\\n95% Confidence Intervals:\")\n",
    "print(f\"  RMSE: [{cv_metrics['rmse_mean'] - t_val * rmse_se:.3f}, {cv_metrics['rmse_mean'] + t_val * rmse_se:.3f}]\\")\n",
    "print(f\"  MAE: [{cv_metrics['mae_mean'] - t_val * mae_se:.3f}, {cv_metrics['mae_mean'] + t_val * mae_se:.3f}]\\")\n",
    "print(f\"  R²: [{cv_metrics['r_squared_mean'] - t_val * r2_se:.3f}, {cv_metrics['r_squared_mean'] + t_val * r2_se:.3f}]\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate external validation recommendations\n",
    "print(\"EXTERNAL VALIDATION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify validation priorities\n",
    "high_priority_strategies = []\n",
    "for result in validation_results:\n",
    "    priority_score = 0\n",
    "    \n",
    "    # Higher priority for poor performance\n",
    "    if result['r_squared'] < 0.5:\n",
    "        priority_score += 3\n",
    "    elif result['r_squared'] < 0.7:\n",
    "        priority_score += 2\n",
    "    else:\n",
    "        priority_score += 1\n",
    "        \n",
    "    if abs(result['bias']) > 0.05:\n",
    "        priority_score += 2\n",
    "    if result['mape'] > 20:\n",
    "        priority_score += 1\n",
    "    \n",
    "    high_priority_strategies.append({\n",
    "        'strategy': result['strategy'],\n",
    "        'priority_score': priority_score,\n",
    "        'r_squared': result['r_squared'],\n",
    "        'bias': result['bias'],\n",
    "        'mape': result['mape']\n",
    "    })\n",
    "\n",
    "# Sort by priority\n",
    "high_priority_strategies.sort(key=lambda x: x['priority_score'], reverse=True)\n",
    "\n",
    "print(\"HIGH PRIORITY VALIDATION NEEDS (in order of priority):\\n\" + \"-\"*50)\n",
    "for i, item in enumerate(high_priority_strategies):\n",
    "    print(f\"{i+1}. {item['strategy']}: Priority Score {item['priority_score']}\")\n",
    "    print(f\"     R²: {item['r_squared']:.3f}, Bias: {item['bias']:.3f}, MAPE: {item['mape']:.2f}%\")\n",
    "    \n",
    "    issues = []\n",
    "    if item['r_squared'] < 0.5:\n",
    "        issues.append(\"Low R² (<0.5)\")\n",
    "    if abs(item['bias']) > 0.05:\n",
    "        issues.append(f\"High bias ({item['bias']:.3f})\")\n",
    "    if item['mape'] > 20:\n",
    "        issues.append(f\"High MAPE ({item['mape']:.1f}%)\")\n",
    "    print(f\"     Issues: {', '.join(issues) if issues else 'None'}\")\n",
    "    print()\n",
    "\n",
    "# Recommendations for improving validation\n",
    "print(\"GENERAL RECOMMENDATIONS FOR MODEL VALIDATION:\\n\" + \"-\"*50)\n",
    "print(\"1. Collect more external validation data, especially for high-priority strategies\\")\n",
    "print(\"2. Investigate systematic biases and recalibrate the model if necessary\\")\n",
    "print(\"3. Consider stratified validation by patient characteristics\\")\n",
    "print(\"4. Perform ongoing validation as new external data becomes available\\")\n",
    "print(\"5. Establish formal validation protocols and acceptance criteria\\")\n",
    "print(\"6. Document validation limitations and uncertainty ranges\\")\n",
    "\n",
    "# Validation confidence levels\n",
    "print(f\"\\nMODEL VALIDATION CONFIDENCE LEVELS:\\n\" + \"-\"*40)\n",
    "for result in validation_results:\n",
    "    confidence = 'High' if result['r_squared'] >= 0.8 else 'Medium' if result['r_squared'] >= 0.6 else 'Low'\n",
    "    print(f\"  {result['strategy']}: {confidence} confidence (R²={result['r_squared']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Collect additional external datasets for validation\n",
    "2. Perform validation across different populations and settings\n",
    "3. Implement automated validation pipelines\n",
    "4. Establish formal validation criteria for model acceptance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}